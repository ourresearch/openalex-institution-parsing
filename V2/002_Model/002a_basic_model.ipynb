{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc67f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import unidecode\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import Counter\n",
    "from math import ceil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f34955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace library to train a tokenizer\n",
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df783e",
   "metadata": {},
   "source": [
    "### Combining the training data from 001 notebook and artificial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f54477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All training samples that have less than 50 different version of the affiliation text\n",
    "# ---- Created in previous notebook\n",
    "lower_than = pd.read_parquet(\"lower_than_50.parquet\")\n",
    "\n",
    "# All training samples that have more than 50 different version of the affiliation text\n",
    "# ---- Created in previous notebook\n",
    "more_than = pd.read_parquet(\"more_than_50.parquet\")\n",
    "\n",
    "print(lower_than.shape)\n",
    "print(more_than.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f756a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_affs_data = pd.concat([more_than, lower_than], \n",
    "                           axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb390e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_affs_data.to_parquet(\"full_affs_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1035dc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6726660, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_affs_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc06758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_affs_data['text_len'] = full_affs_data['original_affiliation'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d923ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_affs_data = full_affs_data[full_affs_data['text_len'] < 500][['original_affiliation','affiliation_id']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cccad5f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6726433, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_affs_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ca1aae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_affs_data['affiliation_id'] = full_affs_data['affiliation_id'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e12f21c",
   "metadata": {},
   "source": [
    "### Processing and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "237023d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_affs_data['processed_text'] = full_affs_data['original_affiliation'].apply(unidecode.unidecode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22e8bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(full_affs_data, train_size=0.985, random_state=1)\n",
    "train_data = train_data.reset_index(drop=True).copy()\n",
    "val_data = val_data.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f94e57e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6625536, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81281ab0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100897, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ee2242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "affs_list_train = train_data['processed_text'].tolist()\n",
    "affs_list_val = val_data['processed_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee7ec70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.system(\"rm aff_text.txt\")\n",
    "    print(\"Done\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83afeec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the affiliation text that will be used to train a tokenizer\n",
    "with open(\"aff_text.txt\", \"w\") as f:\n",
    "    for aff in affs_list_train:\n",
    "        f.write(f\"{aff}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "331b14ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    os.system(\"rm basic_model_tokenizer\")\n",
    "    print(\"Done\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "396120a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_affs_data[['processed_text','affiliation_id']].to_parquet(\"full_affs_data_processed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90e2b7",
   "metadata": {},
   "source": [
    "### Creating the tokenizer for the basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "724890a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordpiece_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# NFD Unicode, lowercase, and getting rid of accents (to make sure text is as readable as possible)\n",
    "wordpiece_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "# Splitting on whitespace\n",
    "wordpiece_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Training a tokenizer on the training dataset\n",
    "trainer = WordPieceTrainer(vocab_size=3816, special_tokens=[\"[UNK]\"])\n",
    "files = [\"aff_text.txt\"]\n",
    "wordpiece_tokenizer.train(files, trainer)\n",
    "\n",
    "wordpiece_tokenizer.save(\"basic_model_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e4b4b",
   "metadata": {},
   "source": [
    "### Further processing of data with tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a923dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_len_and_pad(tok_sent):\n",
    "    \"\"\"\n",
    "    Truncates sequences with length higher than max_len and also pads the sequence\n",
    "    with zeroes up to the max_len.\n",
    "    \"\"\"\n",
    "    max_len = 128\n",
    "    tok_sent = tok_sent[:max_len]\n",
    "    tok_sent = tok_sent + [0]*(max_len - len(tok_sent))\n",
    "    return tok_sent\n",
    "\n",
    "def create_affiliation_vocab(x):\n",
    "    \"\"\"\n",
    "    Checks if affiliation is in vocab and if not, adds to the vocab.\n",
    "    \"\"\"\n",
    "    if x not in affiliation_vocab.keys():\n",
    "        affiliation_vocab[x]=len(affiliation_vocab)\n",
    "    return [affiliation_vocab[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "deb6cb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing an empty affiliation vocab\n",
    "affiliation_vocab = {}\n",
    "\n",
    "# tokenizing the training dataset\n",
    "tokenized_output = []\n",
    "for i in affs_list_train:\n",
    "    tokenized_output.append(wordpiece_tokenizer.encode(i).ids)\n",
    "    \n",
    "train_data['original_affiliation_tok'] = tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37d80c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the validation dataset\n",
    "tokenized_output = []\n",
    "for i in affs_list_val:\n",
    "    tokenized_output.append(wordpiece_tokenizer.encode(i).ids)\n",
    "    \n",
    "val_data['original_affiliation_tok'] = tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f840adc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying max length cutoff and padding\n",
    "train_data['original_affiliation_model_input'] = train_data['original_affiliation_tok'].apply(max_len_and_pad)\n",
    "val_data['original_affiliation_model_input'] = val_data['original_affiliation_tok'].apply(max_len_and_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "331f34e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the label affiliation vocab\n",
    "train_data['label'] = train_data['affiliation_id'].apply(lambda x: create_affiliation_vocab(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f9e15f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102392"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(affiliation_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "afdf7620",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data['label'] = val_data['affiliation_id'].apply(lambda x: [affiliation_vocab.get(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bee93ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_parquet(\"train_data.parquet\")\n",
    "val_data.to_parquet(\"val_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6994186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the affiliation vocab\n",
    "with open(\"affiliation_vocab.pkl\",\"wb\") as f:\n",
    "    pickle.dump(affiliation_vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e469e1",
   "metadata": {},
   "source": [
    "### Creating TFRecords from the training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "479533d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet(\"./train_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98e2dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = pd.read_parquet(\"./val_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c094892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the affiliation vocab\n",
    "with open(\"affiliation_vocab.pkl\",\"rb\") as f:\n",
    "    affiliation_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e56231dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecords_dataset(data, iter_num, dataset_type='train'):\n",
    "    \"\"\"\n",
    "    Creates a TF Dataset that can then be saved to a file to make it faster to read\n",
    "    data during training and allow for transferring of data between compute instances.\n",
    "    \"\"\"\n",
    "    ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(data['original_affiliation_model_input'].to_list()),\n",
    "                              tf.data.Dataset.from_tensor_slices(data['label'].to_list())))\n",
    "    \n",
    "    serialized_features_dataset = ds.map(tf_serialize_example)\n",
    "    \n",
    "    filename = f\"./training_data/{dataset_type}/{str(iter_num).zfill(4)}.tfrecord\"\n",
    "    writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "    writer.write(serialized_features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c27f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_serialize_example(f0, f1):\n",
    "    \"\"\"\n",
    "    Serialization function.\n",
    "    \"\"\"\n",
    "    tf_string = tf.py_function(serialize_example, (f0, f1), tf.string)\n",
    "    return tf.reshape(tf_string, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56124ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(features, label):\n",
    "    \"\"\"\n",
    "    Takes in features and outputs them to a serialized string that can be written to\n",
    "    a file using the TFRecord Writer.\n",
    "    \"\"\"\n",
    "    features_list = tf.train.Int64List(value=features.numpy().tolist())\n",
    "    label_list = tf.train.Int64List(value=label.numpy().tolist())\n",
    "    \n",
    "    features_feature = tf.train.Feature(int64_list = features_list)\n",
    "    label_feature = tf.train.Feature(int64_list = label_list)\n",
    "    \n",
    "    features_for_example = {\n",
    "        'features': features_feature,\n",
    "        'label': label_feature\n",
    "    }\n",
    "    \n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=features_for_example))\n",
    "    \n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab12d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure data is in the correct format before going into TFRecord\n",
    "train_data['original_affiliation_model_input'] = train_data['original_affiliation_model_input'] \\\n",
    ".apply(lambda x: np.asarray(x, dtype=np.int64))\n",
    "\n",
    "val_data['original_affiliation_model_input'] = val_data['original_affiliation_model_input'] \\\n",
    ".apply(lambda x: np.asarray(x, dtype=np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b99925a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "os.system(\"mkdir -p ./training_data/train/\")\n",
    "os.system(\"mkdir -p ./training_data/val/\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba1f308",
   "metadata": {},
   "source": [
    "#### Creating the Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255fdbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(ceil(train_data.shape[0]/500000)):\n",
    "    print(i)\n",
    "    low = i*500000\n",
    "    high = (i+1)*500000\n",
    "    create_tfrecords_dataset(train_data.iloc[low:high,:], i, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9caa5",
   "metadata": {},
   "source": [
    "#### Creating the Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(ceil(val_data.shape[0]/60000)):\n",
    "    print(i)\n",
    "    low = i*60000\n",
    "    high = (i+1)*60000\n",
    "    create_tfrecords_dataset(val_data.iloc[low:high,:], i, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6ff48",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac0fa343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    \"\"\"\n",
    "    Parses the TFRecord file.\n",
    "    \"\"\"\n",
    "    feature_description = {\n",
    "        'features': tf.io.FixedLenFeature((128,), tf.int64),\n",
    "        'label': tf.io.FixedLenFeature((1,), tf.int64)\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    features = example['features']\n",
    "    label = example['label'][0]\n",
    "\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c052593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path, data_type='train'):\n",
    "    \"\"\"\n",
    "    Takes in a path to the TFRecords and returns a TF Dataset to be used for training.\n",
    "    \"\"\"\n",
    "    tfrecords = [f\"{path}{data_type}/{x}\" for x in os.listdir(f\"{path}{data_type}/\") if x.endswith('tfrecord')]\n",
    "    tfrecords.sort()\n",
    "    \n",
    "    \n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=AUTO)\n",
    "    parsed_dataset = raw_dataset.map(_parse_function, num_parallel_calls=AUTO)\n",
    "\n",
    "    parsed_dataset = parsed_dataset.apply(tf.data.experimental.dense_to_ragged_batch(512,drop_remainder=True))\n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "814c8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"./training_data/\"\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "training_data = get_dataset(train_data_path, data_type='train')\n",
    "validation_data = get_dataset(train_data_path, data_type='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b0a6a",
   "metadata": {},
   "source": [
    "### Load Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a907216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the affiliation (target) vocab\n",
    "with open(\"affiliation_vocab.pkl\",\"rb\") as f:\n",
    "    affiliation_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5d53f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_affiliation_vocab = {i:j for j,i in affiliation_vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5ac75e",
   "metadata": {},
   "source": [
    "### Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "950e87ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to tune\n",
    "emb_size = 256\n",
    "max_len = 128\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "dense_1 = 2048\n",
    "dense_2 = 1024\n",
    "learn_rate = 0.00004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ecbe2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, curr_lr):\n",
    "    \"\"\"\n",
    "    Setting up a exponentially decaying learning rate.\n",
    "    \"\"\"\n",
    "    rampup_epochs = 2\n",
    "    exp_decay = 0.17\n",
    "    def lr(epoch, beg_lr, rampup_epochs, exp_decay):\n",
    "        if epoch < rampup_epochs:\n",
    "            return beg_lr\n",
    "        else:\n",
    "            return beg_lr * math.exp(-exp_decay * epoch)\n",
    "    return lr(epoch, start_lr, rampup_epochs, exp_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "191ab1ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3', '/job:localhost/replica:0/task:0/device:GPU:4', '/job:localhost/replica:0/task:0/device:GPU:5', '/job:localhost/replica:0/task:0/device:GPU:6', '/job:localhost/replica:0/task:0/device:GPU:7')\n"
     ]
    }
   ],
   "source": [
    "# Allow for use of multiple GPUs\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    # Model Inputs\n",
    "    tokenized_aff_string_ids = tf.keras.layers.Input((128,), dtype=tf.int64, name='tokenized_aff_string_input')\n",
    "\n",
    "    # Embedding Layers\n",
    "    tokenized_aff_string_emb_layer = tf.keras.layers.Embedding(input_dim=3816, \n",
    "                                                               output_dim=int(emb_size), \n",
    "                                                               mask_zero=True, \n",
    "                                                               trainable=True,\n",
    "                                                               name=\"tokenized_aff_string_embedding\")\n",
    "\n",
    "    tokenized_aff_string_embs = tokenized_aff_string_emb_layer(tokenized_aff_string_ids)\n",
    "        \n",
    "    # First dense layer\n",
    "    dense_output = tf.keras.layers.Dense(int(dense_1), activation='relu', \n",
    "                                             kernel_regularizer='L2', name=\"dense_1\")(tokenized_aff_string_embs)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_1\")(dense_output)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_1\")(dense_output)\n",
    "    pooled_output = tf.keras.layers.GlobalAveragePooling1D()(dense_output)\n",
    "\n",
    "    # Second dense layer\n",
    "    dense_output = tf.keras.layers.Dense(int(dense_2), activation='relu', \n",
    "                                             kernel_regularizer='L2', name=\"dense_2\")(pooled_output)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_2\")(dense_output)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_2\")(dense_output)\n",
    "\n",
    "    # Last dense layer\n",
    "    final_output = tf.keras.layers.Dense(len(affiliation_vocab), activation='softmax', name='cls')(dense_output)\n",
    "\n",
    "    model = tf.keras.Model(inputs=tokenized_aff_string_ids, outputs=final_output)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learn_rate, beta_1=0.9, \n",
    "                                                     beta_2=0.99),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "    curr_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "    filepath_1 = f\"./models/{curr_date}_{dense_1}d1_{dense_2}d2/\" \\\n",
    "\n",
    "\n",
    "    filepath = filepath_1 + \"model_epoch{epoch:02d}ckpt\"\n",
    "\n",
    "    # Adding in checkpointing\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', \n",
    "                                                          verbose=0, save_best_only=False,\n",
    "                                                          save_weights_only=False, mode='auto',\n",
    "                                                          save_freq='epoch')\n",
    "    \n",
    "    # Adding in early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=4)\n",
    "    \n",
    "    start_lr = float(learn_rate)\n",
    "    \n",
    "    # Adding in a learning rate schedule to decrease learning rate in later epochs\n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "    \n",
    "    callbacks = [model_checkpoint, early_stopping, lr_schedule]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea035a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tokenized_aff_string_input   [(None, 128)]            0         \n",
      " (InputLayer)                                                    \n",
      "                                                                 \n",
      " tokenized_aff_string_embedd  (None, 128, 256)         976896    \n",
      " ing (Embedding)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128, 2048)         526336    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128, 2048)         0         \n",
      "                                                                 \n",
      " layer_norm_1 (LayerNormaliz  (None, 128, 2048)        4096      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " global_average_pooling1d_3   (None, 2048)             0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " layer_norm_2 (LayerNormaliz  (None, 1024)             2048      \n",
      " ation)                                                          \n",
      "                                                                 \n",
      " cls (Dense)                 (None, 102392)            104951800 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,559,352\n",
      "Trainable params: 108,559,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada74e47",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b613d5da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(training_data, epochs=20, validation_data=validation_data, verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "172491df",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(str(history.history), open(f\"{filepath_1}_25EPOCHS_HISTORY.json\", 'w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b07c45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
