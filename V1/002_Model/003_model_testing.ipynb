{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc67f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import redshift_connector\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f34955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import unidecode\n",
    "from langdetect import detect\n",
    "from transformers import create_optimizer, TFAutoModelForSequenceClassification, DistilBertTokenizer\n",
    "from transformers import DataCollatorWithPadding, TFDistilBertForSequenceClassification, PreTrainedTokenizerFast\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redshift credentials for querying the OpenAlex database\n",
    "with open(\"redshift_creds.txt\", \"r\") as f:\n",
    "    host = f.readline()[:-1]\n",
    "    password= f.readline()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a3feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a connection\n",
    "conn = redshift_connector.connect(\n",
    "     host=host,\n",
    "     database='dev',\n",
    "     user='app_user',\n",
    "     password=password\n",
    "  )\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6ff48",
   "metadata": {},
   "source": [
    "### Loading the Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c0475b",
   "metadata": {},
   "source": [
    "##### Make sure the gold_1000.parquet and gold_500.parquet files are in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c830c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Gold 1000 dataset (strings with affiliation ID in MAG/OpenAlex)\n",
    "gold_1000 = pd.read_parquet(\"gold_1000.parquet\")\n",
    "gold_1000['true_affiliation_id'] = gold_1000['true_affiliation_id'].apply(lambda x: [int(i) for i in \n",
    "                                                                                     x.split(\",a\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Gold 500 dataset (strings with empty affiliation IDs in MAG/OpenAlex)\n",
    "gold_500_empty = pd.read_parquet(\"gold_500.parquet\")\n",
    "gold_500_empty['true_affiliation_id'] = gold_500_empty['true_affiliation_id'].apply(lambda x: [int(i) for i in \n",
    "                                                                                     x.split(\",a\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_correct_pred(pred, target_list):\n",
    "    if pred in target_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12737018",
   "metadata": {},
   "source": [
    "#### Getting affiliation ID mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af53402",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select affiliation_id, display_name, city, region, country\n",
    "           from mid.institution\"\"\"\n",
    "\n",
    "cursor.execute(\"ROLLBACK;\")\n",
    "cursor.execute(query)\n",
    "df = cursor.fetch_dataframe()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1dec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_affiliation_dict = df.set_index('affiliation_id').to_dict('index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a568e8a4",
   "metadata": {},
   "source": [
    "#### Getting Other Needed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb852d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File that contains dictionary of countries and some alternate names\n",
    "with open(\"countries.json\", \"r\") as f:\n",
    "    countries_dict = json.load(f)\n",
    "\n",
    "countries_list = []\n",
    "\n",
    "_ = [countries_list.append(j) for j in countries_dict.values()]\n",
    "countries_list_flat = [x for y in countries_list for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942f6009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the flat list file to be used in deployment later on\n",
    "with open(\"countries_list_flat.pkl\", \"wb\") as f:\n",
    "    pickle.dump(countries_list_flat, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa728303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of departments to check for when doing initial string prediction\n",
    "list_of_departments = ['Psychology','Nephrology','Other departments', 'Other Departments', 'Nursing & Midwifery',\n",
    "                       'Literature and Creative Writing','Neuroscience','Engineering','Computer Science',\n",
    "                       'Chemistry','Biology','Medicine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442dce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the departments out to a pickle file\n",
    "with open(\"departments_list.pkl\", \"wb\") as f:\n",
    "    pickle.dump(list_of_departments, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Public file of countries and associated cities\n",
    "# -------> http://download.geonames.org/export/dump/\n",
    "all_countries = pd.read_csv(\"allCountries.txt\", delimiter=\"\\t\", header=None)\n",
    "\n",
    "all_countries.columns = [\"geonameid\",\"name\", \"asciiname\", \"alternatenames\", \"latitude\" , \n",
    "                         \"longitude\", \"feature_class\",\"feature_code\",\"country\",\"cc2\",\"admin1\",\"admin2\",\n",
    "                         \"admin3\",\"admin4\",\"population\",\"elevation\",\"dem\",\"timezone\",\"modification\"]\n",
    "\n",
    "# ISO codes for each country\n",
    "country_codes = pd.read_csv(\"country_codes.txt\", delimiter='\\t') \\\n",
    "[['ISO','Country']]\n",
    "\n",
    "country_codes.columns = ['country','country_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6647bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking to get different combinations of cities and countries to check if string contains no\n",
    "# useful information about the insitution other than the city and country\n",
    "\n",
    "# For example if a affiliation string was \"Barcelona, Spain\" we would want to flag it so that the\n",
    "# model does not try to predict an institution\n",
    "\n",
    "# Only using cities that have a population over 100,000\n",
    "country_solo = all_countries[(all_countries['feature_class']=='P') & \n",
    "              (all_countries['population']>100000)].sort_values(\"population\", ascending=False) \\\n",
    "[[\"name\", \"asciiname\",\"country\",\"population\"]] \\\n",
    ".merge(country_codes, how='left', on='country') \\\n",
    "[['name','country_name']]\n",
    "\n",
    "# Creating additional strings to check for\n",
    "country_solo['city_country'] = country_solo['name'] + \", \" + country_solo['country_name']\n",
    "country_solo['country_country'] = country_solo['country_name'] + \", \" + country_solo['country_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2112fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_country_solo_list = list(set(country_solo['name'].drop_duplicates().to_list() + \n",
    "                                  country_solo['country_name'].drop_duplicates().to_list() + \n",
    "                                  country_solo['city_country'].drop_duplicates().to_list() + \n",
    "                                  country_solo['country_country'].drop_duplicates().to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6703007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing out the list to a file to be used in deployment\n",
    "with open(\"city_country_list.pkl\", \"wb\") as f:\n",
    "    pickle.dump(city_country_solo_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11594578",
   "metadata": {},
   "source": [
    "### Loading the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c31b5",
   "metadata": {},
   "source": [
    "#### Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f49db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_lang = 512\n",
    "language_model_dir = \"language_model/\"\n",
    "language_model = TFAutoModelForSequenceClassification.from_pretrained(language_model_dir)\n",
    "language_model_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\", \n",
    "                                                               return_tensors='tf')\n",
    "data_collator = DataCollatorWithPadding(tokenizer=language_model_tokenizer, \n",
    "                                        return_tensors='tf')\n",
    "\n",
    "with open(f\"{language_model_dir}vocab.pkl\", \"rb\") as f:\n",
    "    language_model_tokenizer_affiliation_vocab = pickle.load(f)\n",
    "    \n",
    "inverse_language_affiliation_vocab = {i:j for j,i in language_model_tokenizer_affiliation_vocab.items()}\n",
    "language_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35193444",
   "metadata": {},
   "source": [
    "#### Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN_basic = 128\n",
    "basic_model_dir = \"basic_model/\"\n",
    "basic_model = tf.keras.models.load_model(f'./{basic_model_dir}')\n",
    "basic_tokenizer = PreTrainedTokenizerFast(tokenizer_file=f\"./{basic_model_dir}basic_tokenizer\")\n",
    "\n",
    "with open(f\"./{basic_model_dir}affiliation_vocab.pkl\", \"rb\") as f:\n",
    "    basic_affiliation_vocab = pickle.load(f)\n",
    "    \n",
    "inverse_basic_affiliation_vocab = {i:j for j,i in basic_affiliation_vocab.items()}\n",
    "basic_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e92e8",
   "metadata": {},
   "source": [
    "### Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec56afe",
   "metadata": {},
   "source": [
    "Looking at using both models at the same time because the individual performance was not as good as expected. This code was refined into the final deployment code seen in the 004 notebook and in the deployment folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a525c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country_in_string(text):\n",
    "    \"\"\"\n",
    "    Looks for countries in the affiliation string to be used in filtering later on.\n",
    "    \"\"\"\n",
    "    countries_in_string = []\n",
    "    _ = [countries_in_string.append(x) for x,y in countries_dict.items() if \n",
    "         np.max([1 if re.search(fr\"\\b{i}\\b\", text) else 0 for i in y]) > 0]\n",
    "    _ = [countries_in_string.append(x) for x,y in countries_dict.items() if \n",
    "         np.max([1 if re.search(fr\"\\b{i}\\b\", text.replace(\".\",\"\")) else 0 for i in y]) > 0]\n",
    "    return list(set(countries_in_string))\n",
    "\n",
    "def preprocess_function_pandas_language(examples):\n",
    "    \"\"\"\n",
    "    Returns a tokenized version of the string for the language model.\n",
    "    \"\"\"\n",
    "    return language_model_tokenizer(examples, truncation=True, padding=True, max_length=MAX_LEN_lang)\n",
    "\n",
    "def country_and_language_aware_prediction_language(orig_aff_string, tok_data, top_k=5):\n",
    "    \"\"\"\n",
    "    Prediction for the language model that looks at country in string as well as\n",
    "    the string language.\n",
    "    \"\"\"\n",
    "    # checking for the string language\n",
    "    try:\n",
    "        string_lang = detect(orig_aff_string)\n",
    "    except:\n",
    "        string_lang = 'en'\n",
    "    \n",
    "    # simple string search for country\n",
    "    countries_in_string = get_country_in_string(orig_aff_string)\n",
    "    comma_split = [x for x in orig_aff_string.split(\",\") if x]\n",
    "    \n",
    "    # logic for changing prediction to -1 (for strings that can't be predicted)\n",
    "    if string_lang in ['fa','ko','zh-cn','zh-tw','ja','uk','ru','vi']:\n",
    "        final_pred = -1\n",
    "        final_score = 0.0\n",
    "    elif ((orig_aff_string.startswith(\"Dep\") | \n",
    "           orig_aff_string.startswith(\"School\")) & \n",
    "          (\",\" not in orig_aff_string) & \n",
    "          (not countries_in_string)):\n",
    "        final_pred = -1\n",
    "        final_score = 0.999\n",
    "    elif ((orig_aff_string.startswith(\"Dep\") | \n",
    "           orig_aff_string.startswith(\"School\")) & \n",
    "          (len(comma_split) < 2) & \n",
    "          (not countries_in_string)):\n",
    "        final_pred = -1\n",
    "        final_score = 0.999\n",
    "    elif orig_aff_string in list_of_departments:\n",
    "        final_pred = -1\n",
    "        final_score = 0.999\n",
    "    elif orig_aff_string in city_country_solo_list:\n",
    "        final_pred = -1\n",
    "        final_score = 0.999\n",
    "    elif re.search(r\"\\b(LIANG|YANG|LIU|et al|XIE|JIA|ZHANG|QU)\\b\", orig_aff_string):\n",
    "        final_pred = -1\n",
    "        final_score = 0.999\n",
    "    else:\n",
    "        # getting the predictions and probabilities\n",
    "        data = data_collator(tok_data)\n",
    "        scores, labels = tf.math.top_k(tf.nn.softmax(\n",
    "            language_model.predict([data['input_ids'], \n",
    "                                    data['attention_mask']]).logits)[0].numpy(), 5)\n",
    "        scores = scores.numpy().tolist()\n",
    "        labels = labels.numpy().tolist()\n",
    "        mapped_labels = [inverse_language_affiliation_vocab[i] for i,j in zip(labels,scores) if \n",
    "                         i!=language_model_tokenizer_affiliation_vocab[-1]]\n",
    "        scores = [j for i,j in zip(labels,scores) if i!=language_model_tokenizer_affiliation_vocab[-1]]\n",
    "        final_pred = mapped_labels[0]\n",
    "        final_score = scores[0]\n",
    "        if mapped_labels[0] < 0:\n",
    "            pass\n",
    "        elif not full_affiliation_dict[mapped_labels[0]]['country']:\n",
    "            pass\n",
    "        else:\n",
    "            if not countries_in_string:\n",
    "                pass\n",
    "            else:\n",
    "                for pred,score in zip(mapped_labels, scores):\n",
    "                    if pred < 0:\n",
    "                        break\n",
    "                    elif not full_affiliation_dict[pred]['country']:\n",
    "                        # trying pass instead of break to give time to find the correct country\n",
    "                        pass\n",
    "                    elif full_affiliation_dict[pred]['country'] in countries_in_string:\n",
    "                        final_pred = pred\n",
    "                        final_score = score\n",
    "                        break\n",
    "                    else:\n",
    "                        pass\n",
    "    return [final_pred,final_score,string_lang]\n",
    "\n",
    "def explore_sent_prediction_language(raw_sentence):\n",
    "    \"\"\"\n",
    "    Takes in a raw sentence and returns the prediction from the language model.\n",
    "    \"\"\"\n",
    "    top_k = 5\n",
    "    \n",
    "    sentence = unidecode.unidecode(raw_sentence)\n",
    "    tokenized_data = preprocess_function_pandas_language([sentence])\n",
    "    pred,score,lang = country_and_language_aware_prediction_language(raw_sentence, tokenized_data, top_k)\n",
    "    \n",
    "    return [pred,score,lang]\n",
    "\n",
    "def max_len_and_pad(tok_sent):\n",
    "    \"\"\"\n",
    "    Processes the basic model data to the correct input length.\n",
    "    \"\"\"\n",
    "    max_len = MAX_LEN_basic\n",
    "    tok_sent = tok_sent[:max_len]\n",
    "    tok_sent = tok_sent + [0]*(max_len - len(tok_sent))\n",
    "    return tok_sent\n",
    "\n",
    "def preprocess_function_pandas_basic(examples):\n",
    "    \"\"\"\n",
    "    Returns a tokenized version of the string for the basic model.\n",
    "    \"\"\"\n",
    "    examples = unidecode.unidecode(examples)\n",
    "    examples = basic_tokenizer.encode(examples)\n",
    "    examples = max_len_and_pad(examples)\n",
    "    \n",
    "    return examples\n",
    "\n",
    "def country_and_language_aware_prediction_basic(orig_aff_string, tok_data, top_k=5):\n",
    "    \"\"\"\n",
    "    Prediction for the basic model that looks at country in string as well as\n",
    "    the string language.\n",
    "    \"\"\"\n",
    "    # checking for the string language\n",
    "    try:\n",
    "        string_lang = detect(orig_aff_string)\n",
    "    except:\n",
    "        string_lang = 'en'\n",
    "        \n",
    "    # getting the predictions and probabilities\n",
    "    scores, labels = tf.math.top_k(basic_model.predict([tok_data]), 2)\n",
    "    scores = scores.numpy()[0].tolist()\n",
    "    labels = labels.numpy()[0].tolist()\n",
    "    \n",
    "    # check if the initial prediction is a -1, if it is use the second high prediction\n",
    "    if labels[0] == basic_affiliation_vocab[-1]:\n",
    "        final_pred = inverse_basic_affiliation_vocab[labels[1]]\n",
    "        final_score = scores[1]\n",
    "    else:\n",
    "        final_pred = inverse_basic_affiliation_vocab[labels[0]]\n",
    "        final_score = scores[0]\n",
    "    \n",
    "    # simple string search for country\n",
    "    countries_in_string = get_country_in_string(orig_aff_string)\n",
    "    comma_split = [x for x in orig_aff_string.split(\",\") if x]\n",
    "    \n",
    "    # logic for changing prediction to -1 (for strings that can't be predicted)\n",
    "    if string_lang in ['fa','ko','zh-cn','zh-tw','ja','uk','ru','vi']:\n",
    "        final_pred = -1\n",
    "        final_score = 0.0\n",
    "    elif ((orig_aff_string.startswith(\"Dep\") | \n",
    "           orig_aff_string.startswith(\"School\")) & \n",
    "          (\",\" not in orig_aff_string) & \n",
    "          (not countries_in_string)):\n",
    "        final_pred = -1\n",
    "        final_score = 0.999\n",
    "    elif ((orig_aff_string.startswith(\"Dep\") | \n",
    "           orig_aff_string.startswith(\"School\")) & \n",
    "          (len(comma_split) < 2) & \n",
    "          (not countries_in_string)):\n",
    "        final_pred = -1\n",
    "        final_score = 0.999\n",
    "    elif orig_aff_string in list_of_departments:\n",
    "        final_pred = -1\n",
    "        final_score = 0.999\n",
    "    elif orig_aff_string in city_country_solo_list:\n",
    "        final_pred = -1\n",
    "        final_score = 0.999\n",
    "    elif re.search(r\"\\b(LIANG|YANG|LIU|et al|XIE|JIA|ZHANG|QU)\\b\", orig_aff_string):\n",
    "        final_pred = -1\n",
    "        final_score = 0.999\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return [final_pred,final_score,string_lang]\n",
    "\n",
    "def explore_sent_prediction_basic(raw_sentence):\n",
    "    \"\"\"\n",
    "    Takes in a raw sentence and returns the prediction from the basic model.\n",
    "    \"\"\"\n",
    "    top_k = 5\n",
    "    \n",
    "    tokenized_data = preprocess_function_pandas_basic(raw_sentence)\n",
    "    pred,score,lang = country_and_language_aware_prediction_basic(raw_sentence, tokenized_data, top_k)\n",
    "    \n",
    "    return [pred,score,lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7bb46",
   "metadata": {},
   "source": [
    "### Gold 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e404e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the gold 1000 dataset\n",
    "data_1000 = gold_1000.copy()\n",
    "\n",
    "# Getting predictions for the language model\n",
    "data_1000['pred_score_language'] = data_1000['raw_affiliation'].apply(explore_sent_prediction_language)\n",
    "data_1000['pred_language'] = data_1000['pred_score_language'].apply(lambda x: x[0])\n",
    "data_1000['score_language'] = data_1000['pred_score_language'].apply(lambda x: x[1])\n",
    "\n",
    "# Getting predictions for the basic model\n",
    "data_1000['pred_score_basic'] = data_1000['raw_affiliation'].apply(explore_sent_prediction_basic)\n",
    "data_1000['pred_basic'] = data_1000['pred_score_basic'].apply(lambda x: x[0])\n",
    "data_1000['score_basic'] = data_1000['pred_score_basic'].apply(lambda x: x[1])\n",
    "\n",
    "# Checking if basic model prediction is correct\n",
    "data_1000['pred_correct_basic'] = data_1000.apply(lambda x: check_for_correct_pred(x.pred_basic,\n",
    "                                                                         x.true_affiliation_id), axis=1)\n",
    "\n",
    "# Checking if language model prediction is correct\n",
    "data_1000['pred_correct_language'] = data_1000.apply(lambda x: check_for_correct_pred(x.pred_language,\n",
    "                                                                         x.true_affiliation_id), axis=1)\n",
    "\n",
    "# Checking if the language model and basic model predict the same affiliation\n",
    "data_1000['pred_same'] = data_1000.apply(lambda x: x.pred_basic==x.pred_language, axis=1).astype('int')\n",
    "\n",
    "# Checking if true affiliation should be empty\n",
    "data_1000['equals_negative_one'] = data_1000['true_affiliation_id'] \\\n",
    ".apply(lambda x: x[0]==-1).astype('int')\n",
    "\n",
    "# Checking if true affiliation should not be empty\n",
    "data_1000['not_equals_negative_one'] = data_1000['true_affiliation_id'] \\\n",
    ".apply(lambda x: x[0]!=-1).astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e72634",
   "metadata": {},
   "source": [
    "#### Grid search for the optimal combination of thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572e410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for thresh_basic in [0.0, 0.05, 0.10,0.15,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7,0.8]:\n",
    "    for thresh_language in [0.0, 0.05, 0.1, 0.3,0.6,0.7,0.8,0.9,0.95,0.96,0.98,0.99]:\n",
    "        final_pred_data = pd.DataFrame()\n",
    "        \n",
    "        # Getting all predictions that were matched by both models\n",
    "        match_data_1000 = data_1000[data_1000['pred_same']==1] \\\n",
    "        [['paper_id','true_affiliation_id','raw_affiliation',\n",
    "          'pred_basic','pred_correct_basic']].copy()\n",
    "        match_data_1000.columns = ['paper_id','true_affiliation_id',\n",
    "                                   'raw_affiliation','pred','pred_correct']\n",
    "        match_data_1000['data_type'] = 'match'\n",
    "        match_data_id_list = match_data_1000['paper_id'].to_list()\n",
    "        final_pred_data = pd.concat([final_pred_data, match_data_1000], axis=0)\n",
    "\n",
    "        # Getting all predictions that meet the basic model threshold\n",
    "        basic_thresh_1000 = data_1000[(data_1000['score_basic']>thresh_basic) & \n",
    "                                      (~data_1000['paper_id'].isin(match_data_id_list))]\\\n",
    "        [['paper_id','true_affiliation_id',\n",
    "          'raw_affiliation','pred_basic','pred_correct_basic']].copy()\n",
    "        basic_thresh_1000.columns = ['paper_id','true_affiliation_id',\n",
    "                                     'raw_affiliation','pred','pred_correct']\n",
    "        basic_thresh_1000['data_type'] = 'basic_thresh'\n",
    "        final_pred_data = pd.concat([final_pred_data, basic_thresh_1000], axis=0)\n",
    "        match_data_id_list += basic_thresh_1000['paper_id'].to_list()\n",
    "\n",
    "        # Getting all predictions that meet the language model threshold\n",
    "        language_thresh_1000 = data_1000[(data_1000['score_language']>thresh_language) & \n",
    "                                      (~data_1000['paper_id'].isin(match_data_id_list))]\\\n",
    "        [['paper_id','true_affiliation_id','raw_affiliation',\n",
    "          'pred_language','pred_correct_language']].copy()\n",
    "        language_thresh_1000.columns = ['paper_id','true_affiliation_id',\n",
    "                                        'raw_affiliation','pred','pred_correct']\n",
    "        language_thresh_1000['data_type'] = 'language_thresh'\n",
    "        final_pred_data = pd.concat([final_pred_data, language_thresh_1000], axis=0)\n",
    "        match_data_id_list += language_thresh_1000['paper_id'].to_list()\n",
    "\n",
    "        # Setting the prediction for all leftover strings to the basic model prediction\n",
    "        # These predictions will most likely not be used\n",
    "        last_preds_1000 = data_1000[(~data_1000['paper_id'].isin(match_data_id_list))]\\\n",
    "        [['paper_id','true_affiliation_id',\n",
    "          'raw_affiliation','pred_basic','pred_correct_basic']].copy()\n",
    "        last_preds_1000.columns = ['paper_id','original_affiliation','true_affiliation_id',\n",
    "                                   'raw_affiliation','pred','pred_correct']\n",
    "        last_preds_1000['data_type'] = 'last_preds'\n",
    "        final_pred_data = pd.concat([final_pred_data, last_preds_1000], axis=0)\n",
    "        match_data_id_list += last_preds_1000['paper_id'].to_list()\n",
    "\n",
    "        # Making sure columns are available for new dataframe\n",
    "        final_pred_data['equals_negative_one'] = final_pred_data['true_affiliation_id'] \\\n",
    "        .apply(lambda x: x[0]==-1).astype('int')\n",
    "\n",
    "        final_pred_data['not_equals_negative_one'] = final_pred_data['true_affiliation_id'] \\\n",
    "        .apply(lambda x: x[0]!=-1).astype('int')\n",
    "\n",
    "        # Removing predictions that were in the \"last_preds\"\n",
    "        test_over_1000 = final_pred_data[final_pred_data['data_type']!='last_preds'] \\\n",
    "        [['pred_correct','equals_negative_one','not_equals_negative_one']]\n",
    "\n",
    "        test_under_1000 = final_pred_data[final_pred_data['data_type']=='last_preds'] \\\n",
    "        [['pred_correct','equals_negative_one','not_equals_negative_one']]\n",
    "\n",
    "        # Getting true positives, false positives, and false negatives\n",
    "        TP = test_over_1000[(test_over_1000['pred_correct']==1) & \n",
    "                             (test_over_1000['not_equals_negative_one']==1)].shape[0]\n",
    "        FP =  test_over_1000[(test_over_1000['pred_correct']==0)].shape[0]\n",
    "        FN = test_under_1000[(test_under_1000['not_equals_negative_one']==1)].shape[0]\n",
    "\n",
    "        # Calculating precision and recall\n",
    "        precision = TP/(TP+FP)\n",
    "        recall = TP/(TP+FN)\n",
    "        print(f\"--- {round(thresh_basic, 2)} {round(thresh_language, 2)} --- Precision: {round(precision, 3)}     Recall: {round(recall, 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b24f4",
   "metadata": {},
   "source": [
    "### Gold 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c99af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the gold 500 dataset\n",
    "data_500 = gold_500.copy()\n",
    "\n",
    "# Getting predictions for the language model\n",
    "data_500['pred_score_language'] = data_500['raw_affiliation'].apply(explore_sent_prediction_language)\n",
    "data_500['pred_language'] = data_500['pred_score_language'].apply(lambda x: x[0])\n",
    "data_500['score_language'] = data_500['pred_score_language'].apply(lambda x: x[1])\n",
    "data_500['lang'] = data_500['pred_score_language'].apply(lambda x: x[2])\n",
    "\n",
    "# Getting predictions for the basic model\n",
    "data_500['pred_score_basic'] = data_500['raw_affiliation'].apply(explore_sent_prediction_basic)\n",
    "data_500['pred_basic'] = data_500['pred_score_basic'].apply(lambda x: x[0])\n",
    "data_500['score_basic'] = data_500['pred_score_basic'].apply(lambda x: x[1])\n",
    "\n",
    "# Checking if language model prediction is correct\n",
    "data_500['pred_correct_language'] = data_500.apply(lambda x: check_for_correct_pred(x.pred_language,\n",
    "                                                                            x.true_affiliation_id), axis=1)\n",
    "\n",
    "# Checking if basic model prediction is correct\n",
    "data_500['pred_correct_basic'] = data_500.apply(lambda x: check_for_correct_pred(x.pred_basic,\n",
    "                                                                   x.true_affiliation_id), axis=1)\n",
    "\n",
    "# Checking if the language model and basic model predict the same affiliation\n",
    "data_500['pred_same'] = data_500.apply(lambda x: x.pred_basic==x.pred_language, axis=1).astype('int')\n",
    "\n",
    "# Checking if true affiliation should be empty\n",
    "data_500['equals_negative_one'] = data_500['true_affiliation_id'] \\\n",
    ".apply(lambda x: x[0]==-1).astype('int')\n",
    "\n",
    "# Checking if true affiliation should not be empty\n",
    "data_500['not_equals_negative_one'] = data_500['true_affiliation_id'] \\\n",
    ".apply(lambda x: x[0]!=-1).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91895571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for thresh_basic in [0.0, 0.05, 0.10,0.15,0.2,0.25,0.3,0.35,0.4,0.5,0.6,0.7,0.8]:\n",
    "    for thresh_language in [0.0, 0.05, 0.1, 0.3,0.6,0.7,0.8,0.9,0.95,0.96,0.98,0.99]:\n",
    "        final_pred_data_500 = pd.DataFrame()\n",
    "        \n",
    "        # Getting all predictions that were matched by both models\n",
    "        match_data_500 = data_500[data_500['pred_same']==1] \\\n",
    "        [['paper_id','original_affiliation','true_affiliation_id',\n",
    "          'raw_affiliation','pred_basic','pred_correct_basic']].copy()\n",
    "        match_data_500.columns = ['paper_id','original_affiliation','true_affiliation_id',\n",
    "                                  'raw_affiliation','pred','pred_correct']\n",
    "        match_data_500['data_type'] = 'match'\n",
    "        match_data_id_list_500 = match_data_500['paper_id'].to_list()\n",
    "        final_pred_data_500 = pd.concat([final_pred_data_500, match_data_500], axis=0)\n",
    "\n",
    "        # Getting all predictions that meet the basic model threshold\n",
    "        basic_thresh_500 = data_500[(data_500['score_basic']>thresh_basic) & \n",
    "                                      (~data_500['paper_id'].isin(match_data_id_list_500))]\\\n",
    "        [['paper_id','original_affiliation','true_affiliation_id',\n",
    "          'raw_affiliation','pred_basic','pred_correct_basic']].copy()\n",
    "        basic_thresh_500.columns = ['paper_id','original_affiliation','true_affiliation_id',\n",
    "                                    'raw_affiliation','pred','pred_correct']\n",
    "        basic_thresh_500['data_type'] = 'basic_thresh'\n",
    "        final_pred_data_500 = pd.concat([final_pred_data_500, basic_thresh_500], axis=0)\n",
    "        match_data_id_list_500 += basic_thresh_500['paper_id'].to_list()\n",
    "\n",
    "        # Getting all predictions that meet the language model threshold\n",
    "        language_thresh_500 = data_500[(data_500['score_language']>thresh_language) & \n",
    "                                      (~data_500['paper_id'].isin(match_data_id_list_500))]\\\n",
    "        [['paper_id','original_affiliation','true_affiliation_id',\n",
    "          'raw_affiliation','pred_language','pred_correct_language']].copy()\n",
    "        language_thresh_500.columns = ['paper_id','original_affiliation','true_affiliation_id',\n",
    "                                       'raw_affiliation','pred','pred_correct']\n",
    "        language_thresh_500['data_type'] = 'language_thresh'\n",
    "        final_pred_data_500 = pd.concat([final_pred_data_500, language_thresh_500], axis=0)\n",
    "        match_data_id_list_500 += language_thresh_500['paper_id'].to_list()\n",
    "\n",
    "        # Setting the prediction for all leftover strings to the basic model prediction\n",
    "        # These predictions will most likely not be used\n",
    "        last_preds_500 = data_500[(~data_500['paper_id'].isin(match_data_id_list_500))]\\\n",
    "        [['paper_id','original_affiliation','true_affiliation_id',\n",
    "          'raw_affiliation','pred_basic','pred_correct_basic']].copy()\n",
    "        last_preds_500.columns = ['paper_id','original_affiliation','true_affiliation_id',\n",
    "                                  'raw_affiliation','pred','pred_correct']\n",
    "        last_preds_500['data_type'] = 'last_preds'\n",
    "        final_pred_data_500 = pd.concat([final_pred_data_500, last_preds_500], axis=0)\n",
    "        match_data_id_list_500 += last_preds_500['paper_id'].to_list()\n",
    "\n",
    "        # Making sure columns are available for new dataframe\n",
    "        final_pred_data_500['equals_negative_one'] = final_pred_data_500['true_affiliation_id'] \\\n",
    "        .apply(lambda x: x[0]==-1).astype('int')\n",
    "\n",
    "        final_pred_data_500['not_equals_negative_one'] = final_pred_data_500['true_affiliation_id'] \\\n",
    "        .apply(lambda x: x[0]!=-1).astype('int')\n",
    "\n",
    "        # Removing predictions that were in the \"last_preds\"\n",
    "        test_over = final_pred_data_500[final_pred_data_500['data_type']!='last_preds'] \\\n",
    "        [['pred_correct','equals_negative_one','not_equals_negative_one']]\n",
    "\n",
    "        test_under = final_pred_data_500[final_pred_data_500['data_type']=='last_preds'] \\\n",
    "        [['pred_correct','equals_negative_one','not_equals_negative_one']]\n",
    "\n",
    "        # Getting true positives, false positives, and false negatives\n",
    "        TP = test_over[(test_over['pred_correct']==1) & \n",
    "                             (test_over['not_equals_negative_one']==1)].shape[0]\n",
    "        FP =  test_over[(test_over['pred_correct']==0)].shape[0]\n",
    "        FN = test_under[(test_under['not_equals_negative_one']==1)].shape[0]\n",
    "\n",
    "        # Calculating precision and recall\n",
    "        precision = TP/(TP+FP)\n",
    "        recall = TP/(TP+FN)\n",
    "        \n",
    "        if precision > 0.7:\n",
    "            print(f\"--- {round(thresh_basic, 2)} {round(thresh_language, 2)} --- Precision: {round(precision, 3)}     Recall: {round(recall, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b0d160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
