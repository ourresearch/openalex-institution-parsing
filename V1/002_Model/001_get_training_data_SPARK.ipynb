{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "44920190-9120-4ae8-858f-14f52b073315",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This notebook was created in PySpark (Databricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ba961a2-7e64-4844-8d78-c47e5f47c752",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sc = spark.sparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, StringType, FloatType, ArrayType, DoubleType, StructType, StructField\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5f29191d-1059-4a1a-a42b-0a11e8599c1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# These paths should be changed to wherever you want to save the general data and where you want to save\n",
    "# iteration specific data\n",
    "base_save_path = \"./current_directory/\"\n",
    "iteration_save_path = \"./current_directory/institutional_affiliation_classification/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the credentials for connecting to the OpenAlex DB, which should be replaced\n",
    "# with the url and password for wherever the OpenAlex data is being stored\n",
    "redshift_url = \"\"\n",
    "redshift_password = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "47f9abef-de4b-45bd-8d74-3d30740105f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Getting all data\n",
    "\n",
    "Since all of the data was stored in a Redshift DB, all possible training data was queried from the DB and saved to another location so that everything could be done outside of Redshift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "21919fc2-445a-480a-9465-40f651751af4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# query to grab all data for all papers that have affiliation strings available\n",
    "query = \\\n",
    "\"\"\"\n",
    "select a.original_affiliation, a.affiliation_id, a.match_institution_name, b.ror_id\n",
    "from (select original_affiliation, affiliation_id, match_institution_name\n",
    "from mid.affiliation \n",
    "where original_affiliation is not null) a\n",
    "left join (select affiliation_id, ror_id from mid.institution) b\n",
    "on a.affiliation_id=b.affiliation_id\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "13169071-d5f2-43ea-a332-9e61847ed2a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_data = spark.read \\\n",
    ".format(\"com.databricks.spark.redshift\") \\\n",
    ".option(\"url\", redshift_url) \\\n",
    ".option(\"user\", \"app_user\") \\\n",
    ".option(\"password\", redshift_password) \\\n",
    ".option(\"query\", query) \\\n",
    ".option(\"tempdir\", base_save_path) \\\n",
    ".option(\"forward_spark_s3_credentials\", True) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "79573b0b-f793-4848-b78b-f53504f8665a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving all data to a new location (not necessary for all cases)\n",
    "all_data.write.mode('overwrite').parquet(f\"{base_save_path}all_raw_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting all institutions and ROR IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "810e4e37-bdd6-4382-9f70-2c63e62fd6dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = \\\n",
    "\"\"\"\n",
    "select affiliation_id, ror_id \n",
    "from mid.institution\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b7dd2920-6843-49ce-8e64-f2f72d9a6f8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "institutions = spark.read \\\n",
    ".format(\"com.databricks.spark.redshift\") \\\n",
    ".option(\"url\", redshift_url) \\\n",
    ".option(\"user\", \"app_user\") \\\n",
    ".option(\"password\", redshift_password) \\\n",
    ".option(\"query\", query) \\\n",
    ".option(\"tempdir\", base_save_path) \\\n",
    ".option(\"forward_spark_s3_credentials\", True) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d9b1bf97-2179-44ff-a6db-6f30a8adfadd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving institutions to a new location (not necessary for all cases)\n",
    "institutions.coalesce(1).write.mode('overwrite').parquet(f\"{base_save_path}all_institutions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in data (if saved in another location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bfbb7d3c-7adf-4a23-aaa8-39f6f9f38d7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "institutions = spark.read.parquet(f\"{base_save_path}all_institutions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "06521a7d-58ae-4e31-a5ab-add6547b2145",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "institutions.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "98e62bb7-5d47-484e-9a8f-43a734fd5beb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_data = spark.read.parquet(f\"{base_save_path}all_raw_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ac9c9533-41c5-4474-ba3a-f6cce6e1d2b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_data.cache().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling out a sample of empty affiliations to explore\n",
    "\n",
    "Want to look at some affiliation strings that do not have an institution attached to see if there are any patterns and also see how many of the affiliation strings without an institution could realistically be used to predict an institution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "61125368-3e1e-44c8-9e3f-76ccc1900781",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "empty_affiliations = all_data.filter(F.col('affiliation_id').isNull()).dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c68676c2-226e-40b8-a58f-76df428b8339",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "empty_affiliations \\\n",
    ".withColumn('random_prob', F.rand(seed=20)) \\\n",
    ".orderBy('random_prob') \\\n",
    ".limit(5000) \\\n",
    ".coalesce(1).write.mode('overwrite').parquet(f\"{base_save_path}empty_affiliations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18b42ff7-137e-46f4-a309-4fa30887954a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Getting ROR aff strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0185bff4-2d20-4fce-b3cf-be8fb41386cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This parquet file was created in the \"Exploration\" folder in the 001 notebook\n",
    "ror_data = spark.read.parquet(\"ror_strings.parquet\") \\\n",
    ".select('ror_id','original_affiliation','match_institution_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0f12b85a-4896-4b63-b080-fe74693cd331",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "artificial_data = institutions.dropna(subset=['ror_id']).join(ror_data, how='inner', on='ror_id') \\\n",
    ".select('original_affiliation','affiliation_id','match_institution_name','ror_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering training data\n",
    "\n",
    "Since we are looking at all institutions, we need to up-sample the institutions that don't have many affiliation strings and down-sample the institutions that have large numbers of strings. There was a balance here that needed to be acheived. The more samples that are taken for each institution, the more overall training data we will have and the longer our model will take to train. However, more samples also means more ways of an institution showing up in an affiliation string. The number of samples was set to 50 as it was determined this was a good optimization point based on affiliation string count distribution and time it would take to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fa32b038-5cf3-4c8e-91de-f27c31865b0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "num_samples_to_get = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "da6aa4bc-419c-43dc-80c8-9c84c342f77d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w1 = Window.partitionBy('affiliation_id')\n",
    "\n",
    "# Using the window function to get the affiliation count that will be used to filter later\n",
    "filled_affiliations = all_data \\\n",
    "    .union(artificial_data.select(*all_data.columns)) \\\n",
    "    .dropDuplicates() \\\n",
    "    .filter(~F.col('affiliation_id').isNull()) \\\n",
    "    .dropDuplicates() \\\n",
    "    .withColumn('random_prob', F.rand(seed=20)) \\\n",
    "    .withColumn('id_count', F.count(F.col('affiliation_id')).over(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6f6dfa49-c78f-4c22-accd-2ce0df6bcfeb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Getting all affiliation IDs that have less than 50 unique affiliation strings\n",
    "less_than = filled_affiliations.dropDuplicates(subset=['affiliation_id',\n",
    "                                                       'match_institution_name']) \\\n",
    ".filter(F.col('id_count') < num_samples_to_get).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "49de18b8-1759-47f3-9fc7-b6f3796f0ae1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creates a new dataframe of up-sampled rows of affiliation data for training\n",
    "temp_df_list = []\n",
    "for aff_id in less_than.drop_duplicates(subset=['affiliation_id'])['affiliation_id'].to_list():\n",
    "    temp_df = less_than[less_than['affiliation_id']==aff_id].sample(num_samples_to_get, replace=True)\n",
    "    temp_df_list.append(temp_df)\n",
    "less_than_df = pd.concat(temp_df_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29fb74a2-19bf-4e86-83cd-4c2db32b07b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving data to location\n",
    "less_than_df[['original_affiliation', 'affiliation_id']] \\\n",
    ".to_parquet(f\"{base_save_path}lower_than_{num_samples_to_get}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d4fcd7f5-d83f-4209-a0a4-93a41d3b1db0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "w1 = Window.partitionBy('affiliation_id').orderBy('random_prob')\n",
    "\n",
    "# Getting all affiliation IDs that have 50 or more unique affiliation strings\n",
    "more_than = filled_affiliations.filter(F.col('id_count') >= num_samples_to_get) \\\n",
    ".withColumn('row_number', F.row_number().over(w1)) \\\n",
    ".filter(F.col('row_number') <= num_samples_to_get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4e2a8892-ab40-47a0-92e1-7d1633068c2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Saving data to location\n",
    "more_than.select('original_affiliation', 'affiliation_id') \\\n",
    ".coalesce(1).write.mode('overwrite').parquet(f\"{base_save_path}more_than_{num_samples_to_get}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "df6f90de-19dd-40d2-b9fb-c9039186daa5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "institutional_affiliation_classification_V2",
   "notebookOrigID": 1019775894451973,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
