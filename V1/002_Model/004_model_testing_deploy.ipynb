{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e1e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import unidecode\n",
    "import redshift_connector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from langdetect import detect\n",
    "from transformers import TFAutoModelForSequenceClassification, DistilBertTokenizer\n",
    "from transformers import DataCollatorWithPadding, PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bdeb54",
   "metadata": {},
   "source": [
    "### Getting all affiliations and also affiliation counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93a32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redshift credentials for querying the OpenAlex database\n",
    "with open(\"redshift_creds.txt\", \"r\") as f:\n",
    "    host = f.readline()[:-1]\n",
    "    password= f.readline()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a3feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a connection\n",
    "conn = redshift_connector.connect(\n",
    "     host=host,\n",
    "     database='dev',\n",
    "     user='app_user',\n",
    "     password=password\n",
    "  )\n",
    "\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select affiliation_id, display_name, city, region, country\n",
    "           from mid.institution\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c43cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"ROLLBACK;\")\n",
    "cursor.execute(query)\n",
    "df = cursor.fetch_dataframe()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa1900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"select affiliation_id, count(affiliation_id)\n",
    "           from mid.affiliation\n",
    "           group by affiliation_id\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab27898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(\"ROLLBACK;\")\n",
    "cursor.execute(query)\n",
    "weights = cursor.fetch_dataframe().dropna()\n",
    "weights['affiliation_id'] = weights['affiliation_id'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7bcee",
   "metadata": {},
   "source": [
    "### Using the exact code that will be used for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1695a97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the path\n",
    "prefix = './path_to_model_files/' # insert path to model files here\n",
    "model_path = os.path.join(prefix, 'model_files')\n",
    "\n",
    "# Load the needed files\n",
    "with open(os.path.join(model_path, \"departments_list.pkl\"), \"rb\") as f:\n",
    "    departments_list = pickle.load(f)\n",
    "\n",
    "print(\"Loaded list of departments\")\n",
    "\n",
    "with open(os.path.join(model_path, \"full_affiliation_dict.pkl\"), \"rb\") as f:\n",
    "    full_affiliation_dict = pickle.load(f)\n",
    "\n",
    "print(\"Loaded affiliation dictionary\")\n",
    "\n",
    "with open(os.path.join(model_path, \"countries_list_flat.pkl\"), \"rb\") as f:\n",
    "    countries_list_flat = pickle.load(f)\n",
    "\n",
    "print(\"Loaded flat list of countries\")\n",
    "\n",
    "with open(os.path.join(model_path, \"countries.json\"), \"r\") as f:\n",
    "    countries_dict = json.load(f)\n",
    "\n",
    "print(\"Loaded countries dictionary\")\n",
    "\n",
    "with open(os.path.join(model_path, \"city_country_list.pkl\"), \"rb\") as f:\n",
    "    city_country_list = pickle.load(f)\n",
    "\n",
    "print(\"Loaded strings of city/country combinations\")\n",
    "\n",
    "with open(os.path.join(model_path, \"affiliation_vocab_basic.pkl\"), \"rb\") as f:\n",
    "    affiliation_vocab_basic = pickle.load(f)\n",
    "    \n",
    "inverse_affiliation_vocab_basic = {i:j for j,i in affiliation_vocab_basic.items()}\n",
    "\n",
    "print(\"Loaded basic affiliation vocab\")\n",
    "\n",
    "with open(os.path.join(model_path, \"language_model/vocab.pkl\"), \"rb\") as f:\n",
    "    affiliation_vocab_language = pickle.load(f)\n",
    "\n",
    "inverse_affiliation_vocab_language = {i:j for j,i in affiliation_vocab_language.items()}\n",
    "\n",
    "print(\"Loaded language affiliation vocab\")\n",
    "\n",
    "# Load the tokenizers\n",
    "language_tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\", return_tensors='tf')\n",
    "data_collator = DataCollatorWithPadding(tokenizer=language_tokenizer, \n",
    "                                        return_tensors='tf')\n",
    "\n",
    "basic_tokenizer = PreTrainedTokenizerFast(tokenizer_file=os.path.join(model_path, \"basic_model_tokenizer\"))\n",
    "\n",
    "# Load the models\n",
    "language_model = TFAutoModelForSequenceClassification.from_pretrained(os.path.join(model_path, \"language_model\"))\n",
    "language_model.trainable = False\n",
    "\n",
    "basic_model = tf.keras.models.load_model(os.path.join(model_path, \"basic_model\"), compile=False)\n",
    "basic_model.trainable = False\n",
    "\n",
    "def get_country_in_string(text):\n",
    "    \"\"\"\n",
    "    Looks for countries in the affiliation string to be used in filtering later on.\n",
    "    \"\"\"\n",
    "    countries_in_string = []\n",
    "    _ = [countries_in_string.append(x) for x,y in countries_dict.items() if \n",
    "         np.max([1 if re.search(fr\"\\b{i}\\b\", text) else 0 for i in y]) > 0]\n",
    "    _ = [countries_in_string.append(x) for x,y in countries_dict.items() if \n",
    "         np.max([1 if re.search(fr\"\\b{i}\\b\", text.replace(\".\",\"\")) else 0 for i in y]) > 0]\n",
    "    return list(set(countries_in_string))\n",
    "\n",
    "def max_len_and_pad(tok_sent):\n",
    "    \"\"\"\n",
    "    Processes the basic model data to the correct input length.\n",
    "    \"\"\"\n",
    "    max_len = 128\n",
    "    tok_sent = tok_sent[:max_len]\n",
    "    tok_sent = tok_sent + [0]*(max_len - len(tok_sent))\n",
    "    return tok_sent\n",
    "\n",
    "\n",
    "def get_language(orig_aff_string):\n",
    "    \"\"\"\n",
    "    Guesses the language of the affiliation string to be used for filtering later.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        string_lang = detect(orig_aff_string)\n",
    "    except:\n",
    "        string_lang = 'en'\n",
    "        \n",
    "    return string_lang\n",
    "\n",
    "def get_initial_pred(orig_aff_string, string_lang, countries_in_string, comma_split_len):\n",
    "    \"\"\"\n",
    "    Initial hard-coded filtering of the affiliation text to ensure that meaningless strings\n",
    "    and strings in other languages are not given an institution.\n",
    "    \"\"\"\n",
    "    if string_lang in ['fa','ko','zh-cn','zh-tw','ja','uk','ru','vi']:\n",
    "        init_pred = None\n",
    "    elif not str(orig_aff_string).strip():\n",
    "        init_pred = None\n",
    "    elif ((orig_aff_string.startswith(\"Dep\") | \n",
    "           orig_aff_string.startswith(\"School\") | \n",
    "           orig_aff_string.startswith(\"Ministry\")) & \n",
    "          (comma_split_len < 2) & \n",
    "          (not countries_in_string)):\n",
    "        init_pred = None\n",
    "    elif orig_aff_string in departments_list:\n",
    "        init_pred = None\n",
    "    elif orig_aff_string in city_country_list:\n",
    "        init_pred = None\n",
    "    elif re.search(r\"\\b(LIANG|YANG|LIU|et al|XIE|JIA|ZHANG|QU)\\b\", \n",
    "                   orig_aff_string):\n",
    "        init_pred = None\n",
    "    else:\n",
    "        init_pred = 0\n",
    "    return init_pred\n",
    "\n",
    "def get_language_model_prediction(decoded_text, all_countries):\n",
    "    \"\"\"\n",
    "    Preprocesses the decoded text and gets the output labels and scores for the language model.\n",
    "    \"\"\"\n",
    "    lang_tok_data = language_tokenizer(decoded_text, truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    data = data_collator(lang_tok_data)\n",
    "    all_scores, all_labels = tf.math.top_k(tf.nn.softmax(\n",
    "            language_model.predict([data['input_ids'], \n",
    "                                    data['attention_mask']]).logits).numpy(), 5)\n",
    "    \n",
    "    all_scores = all_scores.numpy().tolist()\n",
    "    all_labels = all_labels.numpy().tolist()\n",
    "    \n",
    "    final_preds_scores = []\n",
    "    for scores, labels, countries in zip(all_scores, all_labels, all_countries):\n",
    "        final_pred, final_score = get_final_basic_or_language_model_pred(scores, labels, countries,\n",
    "                                                                         affiliation_vocab_language, \n",
    "                                                                         inverse_affiliation_vocab_language)\n",
    "        final_preds_scores.append([final_pred, final_score])\n",
    "    \n",
    "    return final_preds_scores\n",
    "\n",
    "def get_basic_model_prediction(decoded_text, all_countries):\n",
    "    \"\"\"\n",
    "    Preprocesses the decoded text and gets the output labels and scores for the basic model.\n",
    "    \"\"\"\n",
    "    basic_tok_data = basic_tokenizer(decoded_text)['input_ids']\n",
    "    basic_tok_data = [max_len_and_pad(x) for x in basic_tok_data]\n",
    "    basic_tok_tensor = tf.convert_to_tensor(basic_tok_data, dtype=tf.int64)\n",
    "    all_scores, all_labels = tf.math.top_k(basic_model.predict(basic_tok_data), 5)\n",
    "    \n",
    "    all_scores = all_scores.numpy().tolist()\n",
    "    all_labels = all_labels.numpy().tolist()\n",
    "    \n",
    "    final_preds_scores = []\n",
    "    for scores, labels, countries in zip(all_scores, all_labels, all_countries):\n",
    "        final_pred, final_score = get_final_basic_or_language_model_pred(scores, labels, countries,\n",
    "                                                                         affiliation_vocab_basic, \n",
    "                                                                         inverse_affiliation_vocab_basic)\n",
    "        final_preds_scores.append([final_pred, final_score])\n",
    "    \n",
    "    return final_preds_scores\n",
    "\n",
    "\n",
    "def get_final_basic_or_language_model_pred(scores, labels, countries, vocab, inv_vocab):\n",
    "    \"\"\"\n",
    "    Takes the scores and labels from either model and performs a quick country matching\n",
    "    to see if the country found in the string can be matched to the country of the\n",
    "    predicted institution.\n",
    "    \"\"\"\n",
    "    mapped_labels = [inv_vocab[i] for i,j in zip(labels,scores) if i!=vocab[-1]]\n",
    "    scores = [j for i,j in zip(labels,scores) if i!=vocab[-1]]\n",
    "    final_pred = mapped_labels[0]\n",
    "    final_score = scores[0]\n",
    "    if not full_affiliation_dict[mapped_labels[0]]['country']:\n",
    "        pass\n",
    "    else:\n",
    "        if not countries:\n",
    "            pass\n",
    "        else:\n",
    "            for pred,score in zip(mapped_labels, scores):\n",
    "                if not full_affiliation_dict[pred]['country']:\n",
    "                    # trying pass instead of break to give time to find the correct country\n",
    "                    pass\n",
    "                elif full_affiliation_dict[pred]['country'] in countries:\n",
    "                    final_pred = pred\n",
    "                    final_score = score\n",
    "                    break\n",
    "                else:\n",
    "                    pass\n",
    "    return final_pred, final_score\n",
    "\n",
    "def get_final_prediction(basic_pred_score, lang_pred_score, countries, raw_sentence, lang_thresh, basic_thresh):\n",
    "    \"\"\"\n",
    "    Performs the model comparison and filtering to get the final prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Getting the individual preds and scores for both models\n",
    "    pred_lang, score_lang = lang_pred_score\n",
    "    pred_basic, score_basic = basic_pred_score\n",
    "    \n",
    "    # Logic for combining the two models\n",
    "    if pred_lang == pred_basic:\n",
    "        final_pred = pred_lang\n",
    "        final_score = score_lang\n",
    "        final_cat = 'match'\n",
    "    elif score_basic > basic_thresh:\n",
    "        final_pred = pred_basic\n",
    "        final_score = score_basic\n",
    "        final_cat = 'basic_thresh'\n",
    "    elif score_lang > lang_thresh:\n",
    "        final_pred = pred_lang\n",
    "        final_score = score_lang\n",
    "        final_cat = 'lang_thresh'\n",
    "    elif (score_basic > 0.01) & ('China' in countries) & ('Natural Resource' in raw_sentence):\n",
    "        final_pred = pred_basic\n",
    "        final_score = score_basic\n",
    "        final_cat = 'basic_thresh_second'\n",
    "    else:\n",
    "        final_pred = None\n",
    "        final_score = 0.0\n",
    "        final_cat = 'nothing'\n",
    "    return [final_pred, final_score, final_cat]\n",
    "\n",
    "def raw_data_to_predictions(df, lang_thresh, basic_thresh):\n",
    "    \"\"\"\n",
    "    High level function to go from a raw input dataframe to the final dataframe with affiliation\n",
    "    ID prediction.\n",
    "    \"\"\"\n",
    "    # Implementing the functions above\n",
    "    df['lang'] = df['affiliation_string'].apply(get_language)\n",
    "    df['country_in_string'] = df['affiliation_string'].apply(get_country_in_string)\n",
    "    df['comma_split_len'] = df['affiliation_string'].apply(lambda x: len([i if i else \"\" for i in \n",
    "                                                                          x.split(\",\")]))\n",
    "\n",
    "    # Gets initial indicator of whether or not the string should go through the models\n",
    "    df['affiliation_id'] = df.apply(lambda x: get_initial_pred(x.affiliation_string, x.lang, \n",
    "                                                               x.country_in_string, x.comma_split_len), axis=1)\n",
    "    \n",
    "    # Filter out strings that won't go through the models\n",
    "    to_predict = df[df['affiliation_id']==0.0].drop_duplicates(subset=['affiliation_string']).copy()\n",
    "    to_predict['affiliation_id'] = to_predict['affiliation_id'].astype('int')\n",
    "\n",
    "    # Decode text so only ASCII characters are used\n",
    "    to_predict['decoded_text'] = to_predict['affiliation_string'].apply(unidecode.unidecode)\n",
    "\n",
    "    # Get predictions and scores for each model\n",
    "    to_predict['lang_pred_score'] = get_language_model_prediction(to_predict['decoded_text'].to_list(), \n",
    "                                                                  to_predict['country_in_string'].to_list())\n",
    "    to_predict['basic_pred_score'] = get_basic_model_prediction(to_predict['decoded_text'].to_list(), \n",
    "                                                                to_predict['country_in_string'].to_list())\n",
    "\n",
    "    # Get the final prediction for each affiliation string\n",
    "    to_predict['affiliation_id'] = to_predict.apply(lambda x: \n",
    "                                                    get_final_prediction(x.basic_pred_score, \n",
    "                                                                         x.lang_pred_score, \n",
    "                                                                         x.country_in_string, \n",
    "                                                                         x.affiliation_string, \n",
    "                                                                         lang_thresh, basic_thresh)[0], axis=1)\n",
    "\n",
    "    # Merge predictions to original dataframe to get the same order as the data that was requested\n",
    "    final_df = df[['affiliation_string']].merge(to_predict[['affiliation_string','affiliation_id']], \n",
    "                                                how='left', on='index')\n",
    "    \n",
    "    final_df['affiliation_id'] = final_df['affiliation_id'].fillna(-1).astype('int')\n",
    "    return final_df\n",
    "\n",
    "\n",
    "print(\"Models initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_correct_pred(pred, target_list):\n",
    "    if pred in target_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb6732f",
   "metadata": {},
   "source": [
    "##### Make sure the gold_1000.parquet and gold_500.parquet files are in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a43b701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Gold 500 dataset (strings with empty affiliation IDs in MAG/OpenAlex)\n",
    "data_500 = pd.read_parquet(\"gold_500.parquet\", \n",
    "                           columns=['raw_affiliation','true_affiliation_id']) \\\n",
    "             .rename(columns={'raw_affiliation': 'affiliation_string'})\n",
    "\n",
    "# Attaching the weights to be used for sampling\n",
    "data_500['affiliation_id_for_weights'] = data_500['true_affiliation_id'].apply(lambda x: x[0])\n",
    "data_500 = data_500.merge(weights, how='left', left_on='affiliation_id_for_weights', right_on='affiliation_id')\n",
    "data_500['count'] = data_500['count'].fillna(7).astype('int')\n",
    "data_500 = data_500[['affiliation_string','true_affiliation_id','count']].copy()\n",
    "\n",
    "# Loading the Gold 1000 dataset (strings with affiliation ID in MAG/OpenAlex)\n",
    "data_1000 = pd.read_parquet(\"gold_1000.parquet\", \n",
    "                            columns=['raw_affiliation','true_affiliation_id']) \\\n",
    "             .rename(columns={'raw_affiliation': 'affiliation_string'})\n",
    "\n",
    "# Attaching the weights to be used for sampling\n",
    "data_1000['affiliation_id_for_weights'] = data_1000['true_affiliation_id'].apply(lambda x: x[0])\n",
    "data_1000 = data_1000.merge(weights, how='left', left_on='affiliation_id_for_weights', right_on='affiliation_id')\n",
    "data_1000['count'] = data_1000['count'].fillna(57170).astype('int')\n",
    "data_1000 = data_1000[['affiliation_string','true_affiliation_id','count']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605dc653",
   "metadata": {},
   "source": [
    "#### Looking at multiple runs of sampling to see what the precision and recall would be for the chosen threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc51416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Using 0.99 and 0.75 thresholds after grid search\n",
    "for lang_thresh in [0.99]:\n",
    "    for basic_thresh in [0.75]:\n",
    "        print(f\"Basic: {basic_thresh}   Lang: {lang_thresh}\")\n",
    "        for i in range(15):\n",
    "            # Sampling from the gold datasets to get a distribution of samples similar to real data\n",
    "            sampled_500_preds = data_500.sample(197, weights=data_500['count'])\n",
    "            sampled_1000_preds = data_1000.sample(803, weights=data_1000['count'])\n",
    "            input_df = pd.concat([sampled_500_preds, sampled_1000_preds], axis=0)\n",
    "            \n",
    "            # Getting predictions\n",
    "            final_df = raw_data_to_predictions(input_df, lang_thresh, basic_thresh)\n",
    "\n",
    "            # Filling in empty affiliations with -1 (if any)\n",
    "            final_df['affiliation_id'] = final_df['affiliation_id'].fillna(-1).astype('int')\n",
    "\n",
    "            # Checking if pred matches the label\n",
    "            final_df['pred_correct'] = final_df.apply(lambda x: check_for_correct_pred(x.affiliation_id,\n",
    "                                                                                     x.true_affiliation_id), axis=1)\n",
    "\n",
    "            final_df['equals_negative_one'] = final_df['true_affiliation_id'] \\\n",
    "            .apply(lambda x: x[0]==-1).astype('int')\n",
    "\n",
    "            final_df['not_equals_negative_one'] = final_df['true_affiliation_id'] \\\n",
    "            .apply(lambda x: x[0]!=-1).astype('int')\n",
    "\n",
    "            # Getting true positives, false positives, and false negatives\n",
    "            TP = final_df[(final_df['pred_correct']==1) & \n",
    "                          (final_df['not_equals_negative_one']==1)].shape[0]\n",
    "\n",
    "            TN = final_df[(final_df['pred_correct']==1) & \n",
    "                          (final_df['not_equals_negative_one']==0)].shape[0]\n",
    "\n",
    "            FP =  final_df[(final_df['pred_correct']==0) & \n",
    "                           (final_df['affiliation_id']!=-1)].shape[0]\n",
    "\n",
    "            FN = final_df[(final_df['not_equals_negative_one']==1) & (final_df['affiliation_id']==-1)].shape[0]\n",
    "\n",
    "            # Calculating precision and recall\n",
    "            precision = TP/(TP+FP)\n",
    "            recall = TP/(TP+FN)\n",
    "            print(f\"-------Precision: {round(precision, 3)}     Recall: {round(recall, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc12b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
