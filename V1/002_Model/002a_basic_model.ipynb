{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc67f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "import unidecode\n",
    "import tensorflow as tf\n",
    "import redshift_connector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import Counter\n",
    "from math import ceil\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f34955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace library to train a tokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80df783e",
   "metadata": {},
   "source": [
    "### Combining the training data from 001 notebook and artificial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f76229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affiliation strings with no affiliation\n",
    "empty_affs = pd.read_csv(\"empty_affs_to_train.csv\") \\\n",
    "[['original_affiliation']]\n",
    "empty_affs['affiliation_id'] = -1\n",
    "\n",
    "\n",
    "# Artificially created affiliation strings with no affiliation (created in 001_Exploration notebook)\n",
    "artificial_empty_affs = pd.read_parquet(\"artificial_empty_affs.parquet\") \\\n",
    "[['original_affiliation']]\n",
    "artificial_empty_affs['affiliation_id'] = -1\n",
    "\n",
    "# All training samples that have less than 50 different version of the affiliation text\n",
    "# ---- Created in previous notebook\n",
    "lower_than = pd \\c\n",
    ".read_parquet(\"lower_than_50.parquet\")\n",
    "\n",
    "# All training samples that have more than 50 different version of the affiliation text\n",
    "# ---- Created in previous notebook\n",
    "more_than = pd.read_parquet(\"more_than_50.parquet\")\n",
    "\n",
    "print(empty_affs.shape)\n",
    "print(artificial_empty_affs.shape)\n",
    "print(lower_than.shape)\n",
    "print(more_than.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb390e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_affs_data = pd.concat([artificial_empty_affs, more_than, lower_than, empty_affs], \n",
    "                           axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_affs_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e12f21c",
   "metadata": {},
   "source": [
    "### Processing and splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237023d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_affs_data['processed_text'] = full_affs_data['original_affiliation'].apply(unidecode.unidecode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e8bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(full_affs_data, train_size=0.975, random_state=1)\n",
    "train_data = train_data.reset_index(drop=True).copy()\n",
    "val_data = val_data.reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee2242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "affs_list_train = train_data['processed_text'].tolist()\n",
    "affs_list_val = val_data['processed_text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ec70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.system(\"rm aff_text.txt\")\n",
    "    print(\"Done\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83afeec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the affiliation text that will be used to train a tokenizer\n",
    "with open(\"aff_text.txt\", \"w\") as f:\n",
    "    for aff in affs_list_train:\n",
    "        f.write(f\"{aff}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.system(\"rm basic_model_tokenizer\")\n",
    "    print(\"Done\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396120a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_affs_data.to_parquet(\"full_affs_data_tokenized.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a90e2b7",
   "metadata": {},
   "source": [
    "### Creating the tokenizer for the basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724890a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpiece_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "\n",
    "# NFD Unicode, lowercase, and getting rid of accents (to make sure text is as readable as possible)\n",
    "wordpiece_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "# Splitting on whitespace\n",
    "wordpiece_tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Training a tokenizer on the training dataset\n",
    "trainer = WordPieceTrainer(vocab_size=3816, special_tokens=[\"[UNK]\"])\n",
    "files = [\"aff_text.txt\"]\n",
    "wordpiece_tokenizer.train(files, trainer)\n",
    "\n",
    "wordpiece_tokenizer.save(\"basic_model_tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37e4b4b",
   "metadata": {},
   "source": [
    "### Further processing of data with tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a923dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_len_and_pad(tok_sent):\n",
    "    \"\"\"\n",
    "    Truncates sequences with length higher than max_len and also pads the sequence\n",
    "    with zeroes up to the max_len.\n",
    "    \"\"\"\n",
    "    max_len = 128\n",
    "    tok_sent = tok_sent[:max_len]\n",
    "    tok_sent = tok_sent + [0]*(max_len - len(tok_sent))\n",
    "    return tok_sent\n",
    "\n",
    "def create_affiliation_vocab(x):\n",
    "    \"\"\"\n",
    "    Checks if affiliation is in vocab and if not, adds to the vocab.\n",
    "    \"\"\"\n",
    "    if x not in affiliation_vocab.keys():\n",
    "        affiliation_vocab[x]=len(affiliation_vocab)\n",
    "    return [affiliation_vocab[x]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6994186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing an empty affiliation vocab\n",
    "affiliation_vocab = {}\n",
    "\n",
    "# tokenizing the training dataset\n",
    "tokenized_output = []\n",
    "for i in affs_list_train:\n",
    "    tokenized_output.append(wordpiece_tokenizer.encode(i).ids)\n",
    "    \n",
    "train_data['original_affiliation_tok'] = tokenized_output\n",
    "\n",
    "# tokenizing the validation dataset\n",
    "tokenized_output = []\n",
    "for i in affs_list_val:\n",
    "    tokenized_output.append(wordpiece_tokenizer.encode(i).ids)\n",
    "    \n",
    "val_data['original_affiliation_tok'] = tokenized_output\n",
    "\n",
    "# applying max length cutoff and padding\n",
    "train_data['original_affiliation_model_input'] = train_data['original_affiliation_tok'].apply(max_len_and_pad)\n",
    "val_data['original_affiliation_model_input'] = val_data['original_affiliation_tok'].apply(max_len_and_pad)\n",
    "\n",
    "# creating the label affiliation vocab\n",
    "train_data['label'] = train_data['affiliation_id'].apply(lambda x: create_affiliation_vocab(x))\n",
    "val_data['label'] = val_data['affiliation_id'].apply(lambda x: [affiliation_vocab.get(x)])\n",
    "\n",
    "# saving the affiliation vocab\n",
    "with open(\"affiliation_vocab.pkl\",\"wb\") as f:\n",
    "    pickle.dump(affiliation_vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e469e1",
   "metadata": {},
   "source": [
    "### Creating TFRecords from the training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56231dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecords_dataset(data, iter_num, dataset_type='train'):\n",
    "    \"\"\"\n",
    "    Creates a TF Dataset that can then be saved to a file to make it faster to read\n",
    "    data during training and allow for transferring of data between compute instances.\n",
    "    \"\"\"\n",
    "    ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(data['original_affiliation_model_input'].to_list()),\n",
    "                              tf.data.Dataset.from_tensor_slices(data['label'].to_list())))\n",
    "    \n",
    "    serialized_features_dataset = ds.map(tf_serialize_example)\n",
    "    \n",
    "    filename = f\"./training_data/{dataset_type}/{str(iter_num).zfill(4)}.tfrecord\"\n",
    "    writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "    writer.write(serialized_features_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c27f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_serialize_example(f0, f1):\n",
    "    \"\"\"\n",
    "    Serialization function.\n",
    "    \"\"\"\n",
    "    tf_string = tf.py_function(serialize_example, (f0, f1), tf.string)\n",
    "    return tf.reshape(tf_string, ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56124ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(features, label):\n",
    "    \"\"\"\n",
    "    Takes in features and outputs them to a serialized string that can be written to\n",
    "    a file using the TFRecord Writer.\n",
    "    \"\"\"\n",
    "    features_list = tf.train.Int64List(value=features.numpy().tolist())\n",
    "    label_list = tf.train.Int64List(value=label.numpy().tolist())\n",
    "    \n",
    "    features_feature = tf.train.Feature(int64_list = features_list)\n",
    "    label_feature = tf.train.Feature(int64_list = label_list)\n",
    "    \n",
    "    features_for_example = {\n",
    "        'features': features_feature,\n",
    "        'label': label_feature\n",
    "    }\n",
    "    \n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=features_for_example))\n",
    "    \n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab12d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure data is in the correct format before going into TFRecord\n",
    "train_data['original_affiliation_model_input'] = train_data['original_affiliation_model_input'] \\\n",
    ".apply(lambda x: np.asarray(x, dtype=np.int64))\n",
    "\n",
    "val_data['original_affiliation_model_input'] = val_data['original_affiliation_model_input'] \\\n",
    ".apply(lambda x: np.asarray(x, dtype=np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99925a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"mkdir -p ./training_data/train/\")\n",
    "os.system(\"mkdir -p ./training_data/val/\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba1f308",
   "metadata": {},
   "source": [
    "#### Creating the Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7255fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(ceil(train_data.shape[0]/500000)):\n",
    "    print(i)\n",
    "    low = i*500000\n",
    "    high = (i+1)*500000\n",
    "    create_tfrecords_dataset(train_data.iloc[low:high,:], i, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b9caa5",
   "metadata": {},
   "source": [
    "#### Creating the Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d38f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(ceil(val_data.shape[0]/80000)):\n",
    "    print(i)\n",
    "    low = i*80000\n",
    "    high = (i+1)*80000\n",
    "    create_tfrecords_dataset(val_data.iloc[low:high,:], i, 'val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6ff48",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0fa343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_function(example_proto):\n",
    "    \"\"\"\n",
    "    Parses the TFRecord file.\n",
    "    \"\"\"\n",
    "    feature_description = {\n",
    "        'features': tf.io.FixedLenFeature((128,), tf.int64),\n",
    "        'label': tf.io.FixedLenFeature((1,), tf.int64)\n",
    "    }\n",
    "\n",
    "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
    "\n",
    "    features = example['features']\n",
    "    label = example['label'][0]\n",
    "\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c052593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path, data_type='train'):\n",
    "    \"\"\"\n",
    "    Takes in a path to the TFRecords and returns a TF Dataset to be used for training.\n",
    "    \"\"\"\n",
    "    tfrecords = [f\"{path}{data_type}/{x}\" for x in os.listdir(f\"{path}{data_type}/\") if x.endswith('tfrecord')]\n",
    "    tfrecords.sort()\n",
    "    \n",
    "    \n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecords, num_parallel_reads=AUTO)\n",
    "    parsed_dataset = raw_dataset.map(_parse_function, num_parallel_calls=AUTO)\n",
    "\n",
    "    parsed_dataset = parsed_dataset.apply(tf.data.experimental.dense_to_ragged_batch(512,drop_remainder=True))\n",
    "    return parsed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"./training_data/\"\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "training_data = get_dataset(train_data_path, data_type='train')\n",
    "validation_data = get_dataset(train_data_path, data_type='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b0a6a",
   "metadata": {},
   "source": [
    "### Load Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a907216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the affiliation (target) vocab\n",
    "with open(\"affiliation_vocab.pkl\",\"rb\") as f:\n",
    "    affiliation_vocab_id = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d53f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_affiliation_vocab = {i:j for j,i in affiliation_vocab_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5ac75e",
   "metadata": {},
   "source": [
    "### Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950e87ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters to tune\n",
    "emb_size = 128\n",
    "max_len = 128\n",
    "num_layers = 6\n",
    "num_heads = 8\n",
    "dense_1 = 2048\n",
    "dense_2 = 1024\n",
    "learn_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbe2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, curr_lr):\n",
    "    \"\"\"\n",
    "    Setting up a exponentially decaying learning rate.\n",
    "    \"\"\"\n",
    "    rampup_epochs = 3\n",
    "    exp_decay = 0.17\n",
    "    def lr(epoch, beg_lr, rampup_epochs, exp_decay):\n",
    "        if epoch < rampup_epochs:\n",
    "            return beg_lr\n",
    "        else:\n",
    "            return beg_lr * math.exp(-exp_decay * epoch)\n",
    "    return lr(epoch, start_lr, rampup_epochs, exp_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191ab1ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Allow for use of multiple GPUs\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "    # Model Inputs\n",
    "    tokenized_aff_string_ids = tf.keras.layers.Input((128,), dtype=tf.int64, name='tokenized_aff_string_input')\n",
    "\n",
    "    # Embedding Layers\n",
    "    tokenized_aff_string_emb_layer = tf.keras.layers.Embedding(input_dim=3816, \n",
    "                                                               output_dim=int(emb_size), \n",
    "                                                               mask_zero=True, \n",
    "                                                               trainable=True,\n",
    "                                                               name=\"tokenized_aff_string_embedding\")\n",
    "\n",
    "    tokenized_aff_string_embs = tokenized_aff_string_emb_layer(tokenized_aff_string_ids)\n",
    "        \n",
    "    # First dense layer\n",
    "    dense_output = tf.keras.layers.Dense(int(dense_1), activation='relu', \n",
    "                                             kernel_regularizer='L2', name=\"dense_1\")(tokenized_aff_string_embs)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_1\")(dense_output)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_1\")(dense_output)\n",
    "    pooled_output = tf.keras.layers.GlobalAveragePooling1D()(dense_output)\n",
    "\n",
    "    # Second dense layer\n",
    "    dense_output = tf.keras.layers.Dense(int(dense_2), activation='relu', \n",
    "                                             kernel_regularizer='L2', name=\"dense_2\")(pooled_output)\n",
    "    dense_output = tf.keras.layers.Dropout(0.20, name=\"dropout_2\")(dense_output)\n",
    "    dense_output = tf.keras.layers.LayerNormalization(epsilon=1e-6, name=\"layer_norm_2\")(dense_output)\n",
    "\n",
    "    # Last dense layer\n",
    "    final_output = tf.keras.layers.Dense(len(affiliation_vocab_id), activation='softmax', name='cls')(dense_output)\n",
    "\n",
    "    model = tf.keras.Model(inputs=tokenized_aff_string_ids, outputs=final_output)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learn_rate, beta_1=0.9, \n",
    "                                                     beta_2=0.99),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "    \n",
    "    curr_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "    filepath_1 = f\"./models/{curr_date}_{dense_1}d1_{dense_2}d2/\" \\\n",
    "\n",
    "\n",
    "    filepath = filepath_1 + \"model_epoch{epoch:02d}ckpt\"\n",
    "\n",
    "    # Adding in checkpointing\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', \n",
    "                                                          verbose=0, save_best_only=False,\n",
    "                                                          save_weights_only=False, mode='auto',\n",
    "                                                          save_freq='epoch')\n",
    "    \n",
    "    # Adding in early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=4)\n",
    "    \n",
    "    start_lr = float(learn_rate)\n",
    "    \n",
    "    # Adding in a learning rate schedule to decrease learning rate in later epochs\n",
    "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)\n",
    "    \n",
    "    callbacks = [model_checkpoint, early_stopping, lr_schedule]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea035a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada74e47",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b613d5da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(training_data, epochs=20, validation_data=validation_data, verbose=1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172491df",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(str(history.history), open(f\"{filepath_1}_20EPOCHS_HISTORY.json\", 'w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf56507",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce310de1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
